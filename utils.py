import wandb
import string
import cv2
import numpy as np
from models import GenModel_FC
import math
from PIL import Image as im
# special tokens
START = 0
STOP = 1
PADDING = 2

# + 3 (because of the START, STOP and PADDING)
letter2index = {l : n + 3 for n, l in enumerate(string.ascii_letters)}
index2letter = {n + 3 : l for n, l in enumerate(string.ascii_letters)}


def get_model():
    """Downloads the model artifact from wandb and loads the weights from it into a new generator object.

    Returns:
        (torch.nn.moule): The pretrained generator model.
    """    
    api = wandb.Api()
    artifact = api.artifact('bijin/GANwriting_Reproducibilty_Challenge/GANwriting:v237', type='model')
    model_dir = artifact.download() + '/contran-5000.model'
    
    weights = torch.load(model_dir)
    gen = GenModel_FC(12)
    state_dict = gen.state_dict()

    for key in state_dict.keys():
        state_dict[key] = weights['gen.' + key]
    
    gen.load_state_dict(state_dict)
    return gen

def normalize(img):
    """Normalizes images to the range 0..255.

    Args:
        img (np.array): 3D array of floats.

    Returns:
        img (np.array): 3D array of 8-bit unsigned ints .
    """    
    img = (img - img.min()) / (img.max() - img.min())
    img *= 255
    img = np.uint8(img)
    return img

def convert_and_pad(word):
    """Converts the word to a list of tokens padded to read length 12.

    Args:
        word (string): A string of characters of max length 10.

    Returns:
        List[int]: A list of ints representing the tokens. 
    """    
    new_word = [letter2index[w] for w in word] # Converting each character to its token value 
    new_word = [START] + new_word + [STOP] # START + chars + STOP
    if len(new_word) < 12: # if too short, pad with PADDING token
        new_word.extend([PADDING] * (12 - len(new_word))) 
    return new_word

def preprocess_text(words, max_input_size=10):
    """Converts the each word into a list of tokens, bounded by start and end token. 
    Padding tokens added if necessary to reach max_input_size and splitting if the original word is too long.

    Args:
        words (List[str]): A batch of words as an array of strings.
        max_input_size (int): The max number of tokens in each input

    Returns:
        torch.tensor: A batch of words converted into a tensor.
    """       
    new_words = []
    for w in words:
        w_len = len(w)
        while (w_len > 0):
            new_words.append(convert_and_pad(w[:max_input_size]))
            w = w[max_input_size:]
            w_len -= max_input_size
        
    return torch.from_numpy(np.array(new_words))   

def preprocess_images(imgs):
    """Rescales, resizes and binarizes a batch of images of handwritten words and returns it as a tensor.
    If there are less than 50 images, the original list is shuffled and repeated until 50 is reached.

    Args:
        imgs (np.array[np.uint8]): Original batch of handwritten word image.

    Returns:
        torch.tensor: Preprocessed word image batch.
    """
    new_imgs = []
    for i in imgs:
        i = np.float32(i)
        i = i / 255.0 # Rescaling to 0..1
        i = cv2.resize(i, (216, 64), interpolation=cv2.INTER_CUBIC) # resizing the image for VGG
        i = cv2.cvtColor(i, cv2.COLOR_RGB2GRAY) # Grayscaling the image
        _, i = cv2.threshold(i, 0.5, 1, cv2.THRESH_OTSU) # thresholding with Otsu's method for binarization
        new_imgs.append(i)
    new_imgs = np.array(new_imgs)
    return torch.from_numpy(new_imgs)

def write2canvas(imgs,spaces,indents,imgs_per_line):
    """Takes in all the images generated and writes them all in blank canvases in their correct positions as in the document. 
    One canvas is equivalent to one page in the document. 
    The function returns a list of canvas whose size is equal to the no. of pages in the document.
    
    Args:
        imgs (np.array[np.uint8]): Images generated by the generator
        spaces (dict[int:set]): Position of the spaces in each line.
        indents (dict[int:set]): Position of the indents in each line.
        imgs_per_line (dict[int:int]): Sum of images, spaces and indents in each line.
        
       Returns:
        np.array[np.uint8]: Array of images equivalent to the pages in the document.
    """
    
  w, h = 2500, 2700
  data = np.zeros((h, w, 3), dtype=np.uint8)
  data[0:2700, 0:2500] = [255,255, 255] 
  canvas = im.fromarray(data, 'RGB')
  offset_w, offset_h = 20,20
  offset = 0, 0
  pages = math.ceil(len(imgs_per_line)/30)
  
  out = []
  img = iter(imgs)
  for page in range(pages):
    line =0
    canvas = im.fromarray(data, 'RGB')
    while(offset_h < 2700):
      no_of_words = imgs_per_line[line]
      sdct = spaces[line]
      idct = indents[line]
      for count in range(no_of_words): 
    
        if count in sdct:
          offset_w = offset_w + 20 
          continue

        if count in idct:
          offset_w = offset_w + 80
          continue

        st=im.fromarray(next(img))
        st_w, st_h = st.size
        offset = (offset_w, offset_h)
        canvas.paste(st, offset)
        offset_w =offset_w + st_w
      offset_h =offset_h + 90
      offset_w = 20
      line=line+1
    # canvas.save('page'+str(page)+'.png')
    out.append(np.array(canvas))
  return out

def postprocess_images(imgs, doc):
    #TODO
    pass
